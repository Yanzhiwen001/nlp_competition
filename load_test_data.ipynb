{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import collections\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "import urllib\n",
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from datasets.utils.file_utils import get_datasets_user_agent\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "#import other packages for dataset store\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split test data based on hau, tha, kir\n",
    "total_test = pd.read_csv(\"test.csv\")\n",
    "test_hau = total_test[total_test['ISO639-3']=='hau'] #64\n",
    "test_tha = total_test[total_test['ISO639-3']=='tha'] #69\n",
    "test_kir = total_test[total_test['ISO639-3']=='kir'] #67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save seperately\n",
    "import csv\n",
    "\n",
    "header = [\"'Id', 'ImageURL', 'ISO639-3'\"]\n",
    "test_hau.to_csv(\"images_test_hau.csv\",index=False)\n",
    "test_tha.to_csv(\"images_test_tha.csv\",index=False)\n",
    "test_kir.to_csv(\"images_test_kir.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-27f7657afc698e1f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/Zhiwen Yan/.cache/huggingface/datasets/csv/default-27f7657afc698e1f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5886b4b85ca417cb5c4a34730532687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86a182885ba4a6b880692ed42af365e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f5e9c84bf84117b4fcd546d73a7190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/Zhiwen Yan/.cache/huggingface/datasets/csv/default-27f7657afc698e1f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a176ac34fe53493db303a073cc0e8da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-dd44d63d94961fbe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/Zhiwen Yan/.cache/huggingface/datasets/csv/default-dd44d63d94961fbe/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045394d7bdab43938053c8c16b626bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc86f86eb0114d5e8989e8bafb566257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7ae0ea9c864716819e9744511beac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/Zhiwen Yan/.cache/huggingface/datasets/csv/default-dd44d63d94961fbe/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112f19eaca2f40738e01bd427b10e72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-009d5bab45ad0376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/Zhiwen Yan/.cache/huggingface/datasets/csv/default-009d5bab45ad0376/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a12ef835f26436a9f043e5774c071f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52a85f117d94bc2ba957c5204a24845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7dd069f51345e5acbe87ddd0ba5563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/Zhiwen Yan/.cache/huggingface/datasets/csv/default-009d5bab45ad0376/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245b4bf5c2ae45b5aa3ec875ea06ab61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset_hau = load_dataset('csv', data_files='images_test_hau.csv')\n",
    "test_dataset_tha = load_dataset('csv', data_files='images_test_tha.csv')\n",
    "test_dataset_kir = load_dataset('csv', data_files='images_test_kir.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a449979c3b5e444d93d3e46a175dbd68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#actual download image from url\n",
    "USER_AGENT = get_datasets_user_agent()\n",
    "\n",
    "def fetch_single_image(image_url, timeout=None, retries=0):\n",
    "    request = urllib.request.Request(\n",
    "        image_url,\n",
    "        data=None,\n",
    "        headers={\"user-agent\": USER_AGENT},\n",
    "    )\n",
    "    with urllib.request.urlopen(request, timeout=timeout) as req:\n",
    "        if 'png' in image_url:\n",
    "          png = Image.open(io.BytesIO(req.read())).convert('RGBA')\n",
    "          png.load() # required for png.split()\n",
    "          background = Image.new(\"RGB\", png.size, (255, 255, 255))\n",
    "          background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n",
    "          image_id = str(uuid.uuid4()) # confused about image_id here?\n",
    "          image_path = \"images_kir_test/\" + image_id + \".jpg\"\n",
    "          background.save(image_path, 'JPEG', quality=80)\n",
    "        else:\n",
    "          image = Image.open(io.BytesIO(req.read()))\n",
    "          image_id = str(uuid.uuid4())\n",
    "          image_path = \"images_kir_test/\" + image_id + \".jpg\"\n",
    "          image.save(image_path)\n",
    "    return image_path\n",
    "\n",
    "def fetch_images(batch, num_threads, timeout=None, retries=3):\n",
    "    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        batch[\"image_path\"] = list(executor.map(fetch_single_image_with_args, batch[\"ImageURL\"]))\n",
    "    return batch\n",
    "\n",
    "num_threads = 20\n",
    "test_dataset_kir = test_dataset_kir.map(fetch_images, batched=True, batch_size=100, fn_kwargs={\"num_threads\": num_threads})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset to local\n",
    "test_dataset_kir.save_to_disk(\"dataset_kir_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload data and extract image feature by InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload dataset for hau tha kir\n",
    "dataset = load_from_disk(\"dataset_kir_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Id', 'ImageURL', 'ISO639-3', 'image_path'],\n",
       "        num_rows: 67\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset \n",
    "#note although it is train here, its actually test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the InceptionV3 model\n",
    "# Prepare images features with a pre-trained InceptionV3 model\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache image features\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img) #1 299 299 3\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get unique train(actually test!) images\n",
    "encode_train = sorted(set(dataset['train']['image_path']))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "  batch_features = image_features_extract_model(img) #16 8 8 2048\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3])) #1 64 2048\n",
    "\n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11ec100023c0832803804b166c9e313587998d7e80407ebd90b9748988d89aaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

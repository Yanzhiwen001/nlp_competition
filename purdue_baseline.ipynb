{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import collections\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from PIL import Image\n",
    "import io\n",
    "import urllib\n",
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from datasets.utils.file_utils import get_datasets_user_agent\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "#import other packages for dataset store\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Zhiwen Yan\\AppData\\Local\\Temp\\ipykernel_26024\\2896650489.py:24: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seed value\n",
    "seed_value= 1022\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# 5. Configure a new global `tensorflow` session\n",
    "from keras import backend as K\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images and store dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e267d3887724ef2823d04bc5f63adb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/41.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fc5564418a4d4ba4015ab5c6720707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/15.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset bloom-captioning/tha (download: 168.28 MiB, generated: 2.08 MiB, post-processed: Unknown size, total: 170.36 MiB) to C:/Users/Zhiwen Yan/.cache/huggingface/datasets/sil-ai___bloom-captioning/tha/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19007f5e3c84fbfa5718adc546f9ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/176M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c7fc6e25d5471b80a725a8d4490fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d857f420b2549e396c63d64841eddbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855a723fce1b42029d6b04e7999bcef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2913 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset bloom-captioning downloaded and prepared to C:/Users/Zhiwen Yan/.cache/huggingface/datasets/sil-ai___bloom-captioning/tha/0.0.0/8efe15718b4a50170c9add75b453aec13ec1c5216111d21815428536fe5913ca. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fc8ec92d95496ba1d15bd3f0ae9238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add the relevant ISO code for the language you want to work with.\n",
    "#iso639_3_letter_code = \"hau\"\n",
    "iso639_3_letter_code = \"tha\"\n",
    "#iso639_3_letter_code = \"kir\"\n",
    "\n",
    "# Download the language specific dataset from HF.\n",
    "dataset = load_dataset(\"sil-ai/bloom-captioning\", iso639_3_letter_code, \n",
    "                       use_auth_token=True, download_mode='force_redownload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story'],\n",
       "        num_rows: 58\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story'],\n",
       "        num_rows: 52\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['image_id', 'image_url', 'caption', 'story_id', 'album_id', 'license', 'original_bloom_language_tag', 'index_in_story'],\n",
       "        num_rows: 2913\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what is included in the dataset object.\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6ea6f590e1439eb0cc1955359b3062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a242f31a59a4312bec5ad0a3d3dd625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd6cd139c5e4aa19b4cba797d58735c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#actual download image from url\n",
    "USER_AGENT = get_datasets_user_agent()\n",
    "\n",
    "def fetch_single_image(image_url, timeout=None, retries=0):\n",
    "    request = urllib.request.Request(\n",
    "        image_url,\n",
    "        data=None,\n",
    "        headers={\"user-agent\": USER_AGENT},\n",
    "    )\n",
    "    with urllib.request.urlopen(request, timeout=timeout) as req:\n",
    "        if 'png' in image_url:\n",
    "          png = Image.open(io.BytesIO(req.read())).convert('RGBA')\n",
    "          png.load() # required for png.split()\n",
    "          background = Image.new(\"RGB\", png.size, (255, 255, 255))\n",
    "          background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n",
    "          image_id = str(uuid.uuid4()) # confused about image_id here?\n",
    "          image_path = \"images_kir/\" + image_id + \".jpg\"\n",
    "          background.save(image_path, 'JPEG', quality=80)\n",
    "        else:\n",
    "          image = Image.open(io.BytesIO(req.read()))\n",
    "          image_id = str(uuid.uuid4())\n",
    "          image_path = \"images_kir/\" + image_id + \".jpg\"\n",
    "          image.save(image_path)\n",
    "    return image_path\n",
    "\n",
    "def fetch_images(batch, num_threads, timeout=None, retries=3):\n",
    "    fetch_single_image_with_args = partial(fetch_single_image, timeout=timeout, retries=retries)\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        batch[\"image_path\"] = list(executor.map(fetch_single_image_with_args, batch[\"image_url\"]))\n",
    "    return batch\n",
    "\n",
    "num_threads = 20\n",
    "dataset = dataset.map(fetch_images, batched=True, batch_size=100, fn_kwargs={\"num_threads\": num_threads})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': '2888c885-b6ea-485f-bf09-5cbc7d988920',\n",
       " 'image_url': 'https://bloom-vist.s3.amazonaws.com/%D0%9A%D2%AF%D0%B7%D0%B3%D2%AF/Kuzgu_3_str_Nasynbatova_Svetlana.jpg',\n",
       " 'caption': '– Кел, бөлүнүп издеп көрөлү. Мен ашканадан издейин. А сен конок бөлмөсүн карачы, – деди Айдай.\\nАдилет:\\n– Туура айтасың. Экөөбүз тең бир жерден издесек, убакыт көп кетет.\\nКонок бөлмөсүндө Адилет телевизордун жанын, дивандын үстүн карады. Ачкыч жок. Килемдин асты менен китеп шкафтын ичин карады. Ал жакта да жок экен.',\n",
       " 'story_id': '99d18914-ca50-4a80-9d20-8cb510e644a2',\n",
       " 'album_id': '9278349f-f0cc-4d87-a4b4-0da7992a7552',\n",
       " 'license': 'cc-by-nc',\n",
       " 'original_bloom_language_tag': 'ky',\n",
       " 'index_in_story': 0,\n",
       " 'image_path': 'images_kir/57cbe5f6-3003-4da0-b5de-22958bfdf749.jpg'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check one of the training samples.\n",
    "dataset['train'][0]\n",
    "#checked, image, url, image_path is matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset to local\n",
    "dataset.save_to_disk(\"dataset_kir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload data and extract image feature by InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload dataset\n",
    "dataset = load_from_disk(\"dataset_kir\")\n",
    "test_dataset = load_from_disk(\"dataset_kir_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following steps are no longer needed once you generate numpy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the InceptionV3 model\n",
    "# Prepare images features with a pre-trained InceptionV3 model\n",
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache image features\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img) #1 299 299 3\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 245/245 [06:45<00:00,  1.66s/it]\n",
      "100%|██████████| 4/4 [00:07<00:00,  1.92s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get unique train images\n",
    "# If we can load dataset from this point\n",
    "encode_train = sorted(set(dataset['train']['image_path']))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "  batch_features = image_features_extract_model(img) #16 8 8 2048\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3])) #1 64 2048\n",
    "\n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())\n",
    "    \n",
    "# Get unique test images\n",
    "encode_test = sorted(set(dataset['test']['image_path']))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset_test = tf.data.Dataset.from_tensor_slices(encode_test)\n",
    "image_dataset_test = image_dataset_test.map(\n",
    "  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset_test):\n",
    "  batch_features = image_features_extract_model(img)\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some special tokens and clean up new line characters.\n",
    "train_captions = [f\"<start> {x} <end>\" for x in dataset['train']['caption']]\n",
    "train_captions = [x.replace('\\n', ' ') for x in train_captions]\n",
    "#test_captions = [f\"<start> {x} <end>\" for x in dataset['train']['caption']]\n",
    "#test_captions = [x.replace('\\n', ' ') for x in test_captions]\n",
    "\n",
    "caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> – Кел, бөлүнүп издеп көрөлү. Мен ашканадан издейин. А сен конок бөлмөсүн карачы, – деди Айдай. Адилет: – Туура айтасың. Экөөбүз тең бир жерден издесек, убакыт көп кетет. Конок бөлмөсүндө Адилет телевизордун жанын, дивандын үстүн карады. Ачкыч жок. Килемдин асты менен китеп шкафтын ичин карады. Ал жакта да жок экен. <end>'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will override the default standardization of TextVectorization to preserve\n",
    "# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\n",
    "def standardize(inputs):\n",
    "  inputs = tf.strings.lower(inputs)\n",
    "  return tf.strings.regex_replace(inputs,\n",
    "                                  r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max word count for a caption.\n",
    "max_length = 50\n",
    "# Use the top 5000 words for a vocabulary.\n",
    "vocabulary_size = 15000\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size, #5000\n",
    "    standardize=standardize,\n",
    "    output_sequence_length=max_length)\n",
    "# Learn the vocabulary from the caption data.\n",
    "tokenizer.adapt(caption_dataset)\n",
    "\n",
    "# Create the tokenized vectors\n",
    "cap_vector = caption_dataset.map(lambda x: tokenizer(x))\n",
    "\n",
    "# Create mappings for words to indices and indicies to words.\n",
    "word_to_index = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more data prepare for traning\n",
    "# Create some mas between images, vectors, and captions\n",
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(dataset['train']['image_path'], cap_vector):\n",
    "  img_to_cap_vector[img].append(cap)\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in list(img_to_cap_vector.keys()):\n",
    "  capt_len = len(img_to_cap_vector[imgt])\n",
    "  img_name_train.extend([imgt] * capt_len)\n",
    "  cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "#delete caption for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change these parameters according to your system's configuration\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy files\n",
    "def map_func(img_name, cap):\n",
    "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "  return img_tensor, cap\n",
    "\n",
    "dataset_tf = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset_tf = dataset_tf.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int64]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset_tf = dataset_tf.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset_tf = dataset_tf.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image captioning model\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # score shape == (batch_size, 64, 1)\n",
    "    # This gives you an unnormalized score for each image feature.\n",
    "    score = self.V(attention_hidden_layer)\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder and decoder\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, tokenizer.vocabulary_size())\n",
    "\n",
    "# Training config.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Loss function to use during training.\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up and execute training\n",
    "# Make sure we save checkpoints during training\n",
    "checkpoint_path = \"./checkpoints/train_kir\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  \n",
    "  # restoring the latest checkpoint in checkpoint_path\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  \n",
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "  accuracy = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "          accuracy += train_accuracy(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "  total_accuracy = (accuracy / int(target.shape[1]))\n",
    "  \n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  \n",
    "\n",
    "  return loss, total_loss, total_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.8282\n",
      "Epoch 1 Loss 4.4204 Accuracy 0.0546\n",
      "Time taken for 1 epoch 456.27 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.8795\n",
      "Epoch 2 Loss 4.1657 Accuracy 0.0692\n",
      "Time taken for 1 epoch 393.91 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 3.5622\n",
      "Epoch 3 Loss 4.0728 Accuracy 0.0722\n",
      "Time taken for 1 epoch 363.41 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 3.4693\n",
      "Epoch 4 Loss 3.9161 Accuracy 0.0752\n",
      "Time taken for 1 epoch 360.83 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 3.6012\n",
      "Epoch 5 Loss 3.8238 Accuracy 0.0779\n",
      "Time taken for 1 epoch 425.09 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 3.6057\n",
      "Epoch 6 Loss 3.7044 Accuracy 0.0807\n",
      "Time taken for 1 epoch 365.83 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 3.4711\n",
      "Epoch 7 Loss 3.6098 Accuracy 0.0832\n",
      "Time taken for 1 epoch 487.52 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.9992\n",
      "Epoch 8 Loss 3.5212 Accuracy 0.0854\n",
      "Time taken for 1 epoch 463.03 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 3.0056\n",
      "Epoch 9 Loss 3.4090 Accuracy 0.0874\n",
      "Time taken for 1 epoch 445.94 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.9207\n",
      "Epoch 10 Loss 3.2772 Accuracy 0.0894\n",
      "Time taken for 1 epoch 384.28 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 2.6830\n",
      "Epoch 11 Loss 3.1287 Accuracy 0.0914\n",
      "Time taken for 1 epoch 334.87 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 2.5486\n",
      "Epoch 12 Loss 2.9834 Accuracy 0.0936\n",
      "Time taken for 1 epoch 325.31 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 2.7273\n",
      "Epoch 13 Loss 2.8385 Accuracy 0.0960\n",
      "Time taken for 1 epoch 336.80 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 2.7235\n",
      "Epoch 14 Loss 2.6677 Accuracy 0.0987\n",
      "Time taken for 1 epoch 236.87 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 2.1340\n",
      "Epoch 15 Loss 2.4873 Accuracy 0.1018\n",
      "Time taken for 1 epoch 275.31 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 2.1833\n",
      "Epoch 16 Loss 2.3201 Accuracy 0.1053\n",
      "Time taken for 1 epoch 296.59 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 2.2396\n",
      "Epoch 17 Loss 2.1349 Accuracy 0.1091\n",
      "Time taken for 1 epoch 323.75 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.7797\n",
      "Epoch 18 Loss 1.9774 Accuracy 0.1135\n",
      "Time taken for 1 epoch 349.28 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 1.5955\n",
      "Epoch 19 Loss 1.8280 Accuracy 0.1184\n",
      "Time taken for 1 epoch 344.84 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 1.5267\n",
      "Epoch 20 Loss 1.6789 Accuracy 0.1237\n",
      "Time taken for 1 epoch 336.65 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adjust this depending on how long you want to train\n",
    "EPOCHS = 20\n",
    "\n",
    "# Train our model!\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset_tf):\n",
    "        batch_loss, t_loss, t_accuracy = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        total_accuracy += t_accuracy\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.4f} Accuracy {total_accuracy/num_steps:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our loss\n",
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
    "                                                 -1,\n",
    "                                                 img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        predicted_word = tf.compat.as_text(index_to_word(predicted_id).numpy())\n",
    "        result.append(predicted_word)\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "    #return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for i in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "        grid_size = max(int(np.ceil(len_result/2)), 2)\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
    "        ax.set_title(result[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test dataset\n",
    "test_dataset = load_from_disk(\"dataset_kir_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['train'][1]\n",
    "len(test_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the list with first var is image_id second var is predict caption\n",
    "pred_data = []\n",
    "for i in range(len(test_dataset['train'])):\n",
    "    image = test_dataset['train']['image_path'][i]\n",
    "    idx = test_dataset['train']['image_path'].index(image)\n",
    "    image_id = test_dataset['train']['Id'][idx]\n",
    "    result, attention_plot = predict(image)\n",
    "    predicted_caption = ' '.join(result).replace(' <end>', '')\n",
    "    \"\"\"\n",
    "    for j in predicted_caption:\n",
    "        if j == \"\":\n",
    "            predicted_caption.remove(j)\n",
    "    \"\"\"\n",
    "    pred_data.append([image_id, predicted_caption])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to csv file\n",
    "import csv\n",
    "\n",
    "header = [\"Id\",\"predicted\"]\n",
    "\n",
    "with open('kir_result.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Id': '0961231d-c9a7-4589-b5da-cf37b6016ccc_tha',\n",
       " 'ImageURL': 'https://bloom-vist.s3.amazonaws.com/%E0%B8%AB%E0%B8%A1%E0%B8%B9%E0%B9%88%E0%B8%9A%E0%B9%89%E0%B8%B2%E0%B8%99%E0%B8%AB%E0%B9%89%E0%B8%A7%E0%B8%A2%E0%B9%81%E0%B8%AB%E0%B9%89%E0%B8%87../18.jpg',\n",
       " 'ISO639-3': 'tha',\n",
       " 'image_path': 'images_tha_test/aacedbd2-d836-4d05-aca9-30c28c789b06.jpg'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset['train'][3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11ec100023c0832803804b166c9e313587998d7e80407ebd90b9748988d89aaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
